{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本目录下所有模型都是拿淘宝“情感分类聊天记录”作为训练数据  \n",
    "旨在学习如何使用nlp模型  \n",
    "目标是从朴素贝叶斯学到bert  \n",
    "  \n",
    "期望的结构是：  \n",
    "1、导包  \n",
    "2、数据预处理  \n",
    "3、模型  \n",
    "4、训练及测试  \n",
    "5、使用  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import jieba\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # 从sklearn.feature_extraction.text里导入文本特征向量化模块\n",
    "from sklearn.naive_bayes import MultinomialNB     # 从sklean.naive_bayes里导入朴素贝叶斯模型\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ShayXU/PycharmProjects/A-Repository-for-Machine-Learning/nlp/naive_bayes'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清洗数据\n",
    "def clean(dirty):\n",
    "    dirty = re.sub('http.*?com', '', dirty, flags=re.IGNORECASE)  # 去除urll\n",
    "    dirty = re.sub('<.*?>', '', dirty)       #去除html <标签>\n",
    "    dirty = re.sub('[0-9][0-9]+', '', dirty)\n",
    "    r = \"\\【.*?】+|\\《.*?》+|\\#.*?#+|[.!/_,$&%^*()<>+\"\"'?@|:~{}#]+|[——！\\，。=？、：“”‘’￥……（）《》【】𠃊÷❛]\"\n",
    "#     r = u'[a-zA-Z0-9’!\"#$%&''()*+,-./:;<=>?@，。?★、…【】《》？“”‘’！[\\]^_`{|}~]+'\n",
    "    dirty = re.sub(r,'', dirty, flags=re.IGNORECASE)\n",
    "    \n",
    "    return dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2', '之前朋友买了4X刚刚试了刚刚好'],\n",
       " ['1', '手机号码订单编号'],\n",
       " ['2', '微信号码B5'],\n",
       " ['1', '上午就到啦'],\n",
       " ['1', '顺丰']]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取数据\n",
    "base_dir = '../data_set/taobao'\n",
    "train_dir = os.path.join(base_dir, 'taobao_train.txt')\n",
    "test_dir = os.path.join(base_dir, 'taobao_test.txt')\n",
    "val_dir = os.path.join(base_dir, 'taobao_val.txt')\n",
    "        \n",
    "# 文本嘛，先读成数组好了，正则取下特殊字符。标点其实可以不去对吧。但是这个文本数据都是短句\n",
    "\n",
    "with open(train_dir, encoding=\"utf-8-sig\") as f:\n",
    "    train_data = [item.split('\\t') for item in clean(f.read()).split('\\n')][:-1]\n",
    "with open(test_dir, encoding=\"utf-8-sig\") as f:\n",
    "    test_data = [item.split('\\t') for item in clean(f.read()).split('\\n')][:-1]\n",
    "with open(val_dir, encoding=\"utf-8-sig\") as f:\n",
    "    val_data = [item.split('\\t') for item in clean(f.read()).split('\\n')][:-1]\n",
    "# list(zip(*val_data))[1]\n",
    "# 好像一个变量输入，一个变量输出的格式会更好一些。 或者写一个函数来读数据。\n",
    "train_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['明矾', '白兰', '栽上', '店儿', '了', '好几块', '乚', '❛', 'ଘ', '我以', '提高', '收藏', '÷', '岸', '好了吧', '加微信', '矮牛', '南安', 'i9CPU', '反光', '发下货', '精灵', 'ys', '笨蛋', '不高', '纹', '配', '认', '没动静', '脆枣', '龙海市', '财源', '另一边', '仔糕', '成工', '滑不动', '蓝光', '人家', '陌生', '浮', '一大堆', '个产箱', '信息', '话筒', '饮食', '黑龙江省', '捂', '凤凰', '把握', '平邮', '😛', 'ins', '春季', '元宝区', '一共', '代发', '五星', '返', '追評', '安置', '三百个', '全系列', '全', '坏掉', '一行', '客户', '快点', '👀', '没人', '黑点', '寄', '原因', '几两', '驾校', '单去', '还会', '不二', '豆色', '鳄鱼', '飞侠', '手纸', '种法', '武威市', '盖不紧', '眼眶', '4K', '赶', '精选', '二日', '尊敬', '党', '那就算了', '评己', '章锋', '绳子', '好好', '弟', '不反', '修改', '甘草'] 17999\n"
     ]
    }
   ],
   "source": [
    "# 生成词典\n",
    "word_dict = set()\n",
    "tmp = []\n",
    "tmp.extend(list(zip(*train_data))[1])\n",
    "tmp.extend(list(zip(*test_data))[1])\n",
    "tmp.extend(list(zip(*val_data))[1])\n",
    "\n",
    "for sentence in tmp:\n",
    "    word_dict = word_dict.union(set(jieba.lcut(sentence)))\n",
    "word_dict = list(word_dict)\n",
    "print(word_dict[0:100], len(word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 还要转换成词向量\n",
    "# 最后还要用模型转成句向量\n",
    "\n",
    "# 1、独热码\n",
    "# 要生成词典\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2、词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.数据预处理：训练集和测试集分割，文本特征向量化\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=33) # 随机采样25%的数据样本作为测试集\n",
    "print(X_train)  #查看训练样本\n",
    "print(y_train)  #查看标签\n",
    "\n",
    "#文本特征向量化\n",
    "# vec = CountVectorizer()\n",
    "# X_train = vec.fit_transform(X_train)\n",
    "# X_test = vec.transform(X_test)\n",
    "\n",
    "#3.使用朴素贝叶斯进行训练\n",
    "mnb = MultinomialNB()   # 使用默认配置初始化朴素贝叶斯\n",
    "mnb.fit(X_train,y_train)    # 利用训练数据对模型参数进行估计\n",
    "y_predict = mnb.predict(X_test)     # 对参数进行预测\n",
    "\n",
    "#4.获取结果报告\n",
    "print('The Accuracy of Naive Bayes Classifier is:', mnb.score(X_test,y_test))\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
